# -*- coding: utf-8 -*-
"""Logistic Regression Derivation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gip2WewtO0YIOx6DMpD5-OmgLcr54V5h
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split 

# Creating new data points for testing 
np.random.seed(1)
X, y = datasets.make_blobs(n_samples=1000, centers=2)
y = y[:, np.newaxis]


# Creation of the class
class LogisticRegression:

  def __init__(self, learning_rate=0.001, n_iters=10000):
    self.learning_rate = learning_rate
    self.n_iters = n_iters
    self.w = None
    self.b = None


  def fit(self, X, y):
     # Parameters initialization
    n_samples, n_features = X.shape
    self.w = np.zeros((n_features, 1))
    self.b = 0

    # Gradient descent
    for i in range(self.n_iters):
      y_pred = self.sigmoid_func(np.dot(X, self.w) + self.b)

      dw = (1/n_samples) * np.dot(X.T, (y_pred - y))  # Derivative of w w.r.t. to Loss
      db = (1/n_samples) * np.sum(y_pred - y)  # Deivative of b w.r.t to Loss

      # Updating the weights
      self.w -= self.learning_rate * dw
      self.b -= self.learning_rate * db
    
    return self.w, self.b

  def predict(self, y):
    y_pred = self.sigmoid_func(np.dot(y, self.w) + self.b) # Outputs a probability
    pred_class = [1 if probs > 0.5 else 0 for probs in y_pred ]
    return np.array(pred_class).reshape(-1, 1)

  def sigmoid_func(self, value):
    return 1/(1 + np.exp(-value))


# Testing with the data we created above
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42 )

# Instantiating a class
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

#Predicting the model
predicted_y = log_reg.predict(X_test)

print(f"Accuracy of the model is: {np.mean(predicted_y.reshape(-1, 1) == y_test)*100}%")